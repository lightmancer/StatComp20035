## -----------------------------------------------------------------------------

dose <-c (20,30,35,40,60)
drugA <-c (16,20,27,40,60)

plot(dose,drugA,type="b")

rnames<-c("R1","R2")
cnames<-c("C1","C2","C3","C4","C5")
x <- matrix(1:10,nrow=2,dimnames=list(rnames,cnames)); x


## ----echo=FALSE---------------------------------------------------------------
n <- 1000
u <- runif(n)
x <- 2/(sqrt(1-u))
hist(x,prob=TRUE,main=expression(f(x)==8/x^3),breaks=50)
y <-seq(2,25,0.01)
lines(y,8/y^3,col="red")

## -----------------------------------------------------------------------------
n <- 1000
x1 <- runif(n,-1,1)
x2 <- runif(n,-1,1)
x3 <- runif(n,-1,1)
y <- seq(-1,1,0.01)
z <- 3/4*(1-y^2)
unif <- rep(0, n)


for(i in 1:n)
{
  if(abs(x3[i])>=abs(x2[i]) && abs(x3[i])>=abs(x1[i]))
    {x <- x2[i]
    unif[i] <- x}
  else 
    {x <- x3[i]
    unif[i] <-x}
}
  
hist(unif,prob=TRUE,breaks=50)
lines(y,z,col="red")
  


## -----------------------------------------------------------------------------
n <- 1000
u <- runif(n)
x <- 2*(1-u)^(-0.25)-2
y <- seq(0,10,0.1)
w <- 64/(2+y)^5
hist(x,prob=TRUE,main=expression(f(x)==64/(x+2)^5),breaks = 50)
lines(y, w, col="red")

## -----------------------------------------------------------------------------
n <- 10000
x <- runif(n,0,pi/3)
m <- pi/3*mean(sin(x))
print(c(m,0.5))



## -----------------------------------------------------------------------------
##simple Monte Carlo method
n <- 10000
x <- runif(n,0,1)
m1 <- mean(exp(x))
Var1 <- var(exp(x))/n
##calculate the variance and compared with the theoretical value
Var2 <- (2*exp(1)-0.5*exp(2)-1.5)/n
print(c(Var1,Var2))
##use the antithetic method
y <- runif(n/2,0,1)
z <- 1-y
m2 <- mean(1/2*exp(y)+1/2*exp(z))
print(c(m1,m2))
Var3 <- 2*var(1/2*exp(y)+1/2*exp(z))/n
print(c(Var1,Var3,(Var1-Var3)/Var1))



## -----------------------------------------------------------------------------
m <- 10000
u <- runif(m,0,1)
g <- function(x){x^(-4)*exp(-1/2*(x^(-2)))*(x>0)*(x<1)}
Mean <- mean(g(u))/sqrt(2*pi);Mean
Var <- var((g(u))/sqrt(2*pi));Var

## -----------------------------------------------------------------------------
m <- 10000
x1 <- rexp(m,1)
x2 <- x1+1
g <- function(x){x^2*exp(-0.5*x^2+x)*(x>1)}
Mean <- mean(g(x2))/sqrt(2*pi)/exp(1);Mean
Var <- var((g(x2))/sqrt(2*pi)/exp(1));Var


## -----------------------------------------------------------------------------
m <- 10000
T <- matrix(0,5,2)
for(i in 1:5)
{
  u <- runif(m)
  z <- exp((1-i)/5)-exp(-i/5) 
  x <- -log(exp(-0.2*i+0.2)-(u*z))  # the inverse transform method to generate random numbers
  g <- function(y){z/(1+y^2)*(y>((i-1)/5))*(y<(i/5))}
  data <-g(x)
  Mean <- mean(data)
  T[i,1] <- Mean
  T[i,2] <- var(data)
}
est1 <- sum(T[,1]);est1
est2 <- 5*sum(T[,2]);est2



## -----------------------------------------------------------------------------
n <- 20
alpha <- .05 
meanlog <- 0
sdlog <- 4
# the upper limit of the test
UCL <- replicate(1000, 
                 expr = { x <- rlnorm(n,meanlog,sdlog) 
                 mean(log(x))-qt(alpha/2, df = n-1) 
                 }) 
# the lower limit of the test
LCL <- replicate(1000,
                 expr = { x <- rlnorm(n,meanlog,sdlog) 
                 mean(log(x))-qt((1-alpha)/2, df = n-1) 
                 })
z1 <- sum(LCL<meanlog&&meanlog<UCL)
z2 <- mean(LCL<meanlog&&meanlog<UCL)



## -----------------------------------------------------------------------------
m <- 10000
n <- 50
x <- rchisq(n,2)
LCL <- mean(x)+(sqrt(var(x))*qt(0.025,n-1))
UCL <- mean(x)-(sqrt(var(x))*qt(0.025,n-1))
print(c(LCL,UCL))
j=0
for(i in 1:m)
{
  y <- rchisq(m,2)
  if(LCL<y&&y<UCL)
    j=j+1  
}
j/m

## -----------------------------------------------------------------------------
n <- 50 #sample size
alpha <- 2 # the parameter of the beta distribution
cv1 <- qbeta(0.975,alpha,alpha)
cv2 <- qbeta(0.025,alpha,alpha) #two sides of rejected fields
m <- 10000 # number of replicates
sk <- function(x)  #computes the sample skewness coeff.
{ 
  xbar <- mean(x) 
  m3 <- mean((x - xbar)^3) 
  m2 <- mean((x - xbar)^2) 
  return( m3 / m2^1.5 )
} 
power <- numeric(n)
for(i in 1:n)
{
  tests <- numeric(m)
  for(j in 1:m)
  {
    x <- rbeta(n,alpha,alpha)
    tests[j] <- as.integer(sk(x)>=cv2&&sk(x)<=cv1 ) #modefied rejected field
  }
  power <- mean(tests)
}
print(power)

## -----------------------------------------------------------------------------

n1 <- n2 <- 20
mu1 <- mu2 <- 0
sigma1 <- 1
sigma2 <- 1.5
count5test <- function(x,y) 
{ X <- x - mean(x) 
  Y <- y - mean(y) 
  outx <- sum(X > max(Y)) + sum(X < min(Y)) 
  outy <- sum(Y > max(X)) + sum(Y < min(X)) 
  # return 1 (reject) or 0 (do not reject H0) 
  return(as.integer(max(c(outx, outy)) > 5)) 
}
m <- 10000
power <- mean(replicate(m, 
              expr={ 
                x <- rnorm(n1,mu1,sigma1) 
                y <- rnorm(n2,mu2,sigma2) 
                count5test(x, y) })
              )
print(power)

## -----------------------------------------------------------------------------
n1 <- n2 <- 20
mu1 <- mu2 <- 0
sigma1 <- 1
sigma2 <- 1.5
cv1 <- qf(0.0275,n1-1,n2-1) # small rejected side
cv2 <- qf(0.9725,n1-1,n2-1) # large rejected side
m <- 10000
result <- numeric(m)
for(i in 1:m)
{
  x <- rnorm(n1,mu1,sigma1)
  y <- rnorm(n2,mu2,sigma2)
  f <- var(x)/var(y)
  if((f>=cv2)||(f<=cv1)) result[i]=result[i]+1
}
print(mean(result))

## -----------------------------------------------------------------------------
n <- 20 # the number of sample
m <- 5000 # the number of replicate
d <- 5 # dimention
cv1 <- qchisq(0.975,d*(d+1)*(d+2)/6)
cv2 <- qchisq(0.025,d*(d+1)*(d+2)/6)
p.reject <- numeric(n)
tests <- numeric(m)
for(i in 1:n)
{
  sktests <- numeric(m)
  for(j in 1:m)
  {
    x <- matrix(rnorm(n*d,0,2),ncol=d) 
    m1 <- apply(x,2,mean) #calculate the expectation of col
    mat <- matrix(rep(m1,n),ncol=d)
    D <- x-mat
    Sigma.hat <-var(x)*(n-1)/n
    inv <- solve(Sigma.hat)
    statistic <- sum((D%*%inv%*%t(D))^3)/(n^2)
    chi <- n*statistic/6
    tests[j] <- as.integer(chi<cv2||chi>cv1)
  }
  p.reject[i] <- mean(tests)
}
print(mean(p.reject))

## -----------------------------------------------------------------------------
alpha <-0.1 
n <- 50 # number of sample
m <- 5000 
d <-5 #dimention=5
epsilon <- c(seq(0,0.15,0.01), seq(0.15,1,0.05))    
N <- length(epsilon) 
pwr <- numeric(N) 
#critical value for the skewness test 
cv1 <- qchisq(0.95,d*(d+1)*(d+2)/6)
cv2 <- qchisq(0.05,d*(d+1)*(d+2)/6)
for (j in 1:N) 
{ 
  #for each epsilon 
  e <- epsilon[j] 
  sktests <- numeric(m) 
  for (i in 1:m) 
  { #for each replicate 
    sigma <- sample(c(1, 10), replace = TRUE, size = n, prob = c(1-e, e)) 
    #design the statistic
    x <- matrix(rnorm(n*d,0,sigma),ncol=d) 
    m1 <- apply(x,2,mean) #calculate the expectation of col
    mat <- matrix(rep(m1,n),ncol=d)
    D <- x-mat
    Sigma.hat <-var(x)*(n-1)/n
    inv <- solve(Sigma.hat)
    statistic <- sum((D%*%inv%*%t(D))^3)/(n^2)
    chi <- statistic*n/6
    sktests[i] <- as.integer(chi>cv1) 
  } 
  pwr[j] <- mean(sktests) 
} 
plot(epsilon,pwr,type = "b",ylim = c(0,1)) 
abline(h = .1, lty = 3)
se <- sqrt(pwr * (1-pwr) / m) #add standard errors
lines(epsilon, pwr+se, lty = 3)
lines(epsilon, pwr-se, lty = 3)


## -----------------------------------------------------------------------------


data(law, package = "bootstrap")
n <- nrow(law)
theta.jack <-numeric(n)
for (i in 1:n) 
  {
  data <- law[-i,]
  theta.jack[i] <- cor(data[,1],data[,2])
}
  theta.hat <- cor(law[,1],law[,2])
  bias <- (n - 1) * (mean(theta.jack) - theta.hat)
  print(bias)
 print(sqrt((n-1) * mean((theta.jack - mean(theta.jack))^2)))




## -----------------------------------------------------------------------------

library(boot) #for boot and boot.ci
dat <- aircondit
theta.boot <- function(hours,intd) {
  #function to compute the statistic
  1/mean(hours[intd,1])
}
boot.obj <- boot(dat, statistic = theta.boot, R = 2000)
print(boot.obj)
print(boot.ci(boot.obj,
              type = c("basic", "norm", "perc","bca")))




## ----eval=FALSE---------------------------------------------------------------
#  data(scor,package="bootstrap")
#  #scor itself contains a matirx
#  n <- 1000
#  theta.hat <- numeric(n)
#  for(k in 1:n)
#  { x1<- sample(scor$mec,size=5,replace=T)
#    x2<- sample(scor$vec,size=5,replace=T)
#    x3<- sample(scor$alg,size=5,replace=T)
#    x4<- sample(scor$ana,size=5,replace=T)
#    x5<- sample(scor$sta,size=5,replace=T)
#  MatirX <- matrix(c(x1,x2,x3,x4,x5),byrow=FALSE)
#  Matir.sample <- (n-1)*cov(MatirX)/n
#  ev <- eigen(Matir.sample)
#  evsum <-mean(ev$values)*5
#  lambda1 <- order(ev$values,decreasing=TRUE)[1]
#  theta.hat[k] <- lambda1/evsum
#  }
#  print(mean(theta.hat))
#  theta.jack <- numeric(n)
#  lambda.hat <- numeric(n)
#  for (j in 1:n)
#  {
#  
#    for(i in 1:5)
#      {
#      lambda.hat[j] <- order(ev$values[-i],decreasing=TRUE)[1]
#      theta.jack[j] <- lambda.hat[j] / (mean(ev$values)*(4))
#      }
#  bias <- (n-1) * (mean(theta.jack) - theta.hat)
#  se <- sqrt((n-1) *mean((theta.jack - mean(theta.jack))^2))
#  }
#  print(c(mean(bias),mean(se)))

## ----echo=FALSE---------------------------------------------------------------
library(DAAG)
attach(ironslag)
a <- seq(10, 40, .1) #sequence for plotting fits
L1 <- lm(magnetic ~ chemical)
plot(chemical, magnetic, main="Linear", pch=16)
yhat1 <- L1$coef[1] + L1$coef[2] * a
lines(a, yhat1, lwd=2)
L2 <- lm(magnetic ~ chemical + I(chemical^2))
plot(chemical, magnetic, main="Quadratic", pch=16)
yhat2 <- L2$coef[1] + L2$coef[2] * a + L2$coef[3] * a^2
lines(a, yhat2, lwd=2)
L3 <- lm(log(magnetic) ~ chemical)
plot(chemical, magnetic, main="Exponential", pch=16)
logyhat3 <- L3$coef[1] + L3$coef[2] * a
yhat3 <- exp(logyhat3)
lines(a, yhat3, lwd=2)
L4 <- lm(log(magnetic) ~ log(chemical))
plot(log(chemical), log(magnetic), main="Log-Log", pch=16)
logyhat4 <- L4$coef[1] + L4$coef[2] * log(a)
lines(log(a), logyhat4, lwd=2)

n <- length(magnetic) #in DAAG ironslag
e1 <- e2 <- e3 <- e4 <- numeric(n)
# for n-fold cross validation
# fit models on leave-one-out samples
for (k in 1:n) {
y <- magnetic[-k]
x <- chemical[-k]
J1 <- lm(y ~ x)
yhat1 <- J1$coef[1] + J1$coef[2] * chemical[k]
e1[k] <- magnetic[k] - yhat1
J2 <- lm(y ~ x + I(x^2))
yhat2 <- J2$coef[1] + J2$coef[2] * chemical[k] +
J2$coef[3] * chemical[k]^2
e2[k] <- magnetic[k] - yhat2
J3 <- lm(log(y) ~ x)
logyhat3 <- J3$coef[1] + J3$coef[2] * chemical[k]
yhat3 <- exp(logyhat3)
e3[k] <- magnetic[k] - yhat3
J4 <- lm(log(y) ~ log(x))
logyhat4 <- J4$coef[1] + J4$coef[2] * log(chemical[k])
yhat4 <- exp(logyhat4)
e4[k] <- magnetic[k] - yhat4
}

## ---- eval=FALSE--------------------------------------------------------------
#  # Count Five test
#  count5test = function(x, y) {
#  X = x - mean(x)
#  Y = y - mean(y)
#  outx = sum(X > max(Y)) + sum(X < min(Y))
#  outy = sum(Y > max(X)) + sum(Y < min(X))
#  # return 1 (reject) or 0 (do not reject H0)
#  return(as.integer(max(c(outx, outy)) > 5))
#  }
#  # Count Five test permutation
#  count5test_permutation = function(z)
#  {
#  n = length(z)
#  x = z[1:(n/2)]
#  y = z[-(1:(n/2))]
#  X = x - mean(x)
#  Y = y - mean(y)
#  outx = sum(X > max(Y)) + sum(X < min(Y))
#  outy = sum(Y > max(X)) + sum(Y < min(X))
#  return(as.integer(max(c(outx, outy)) > 5))
#  }
#  permutation = function(z,R)
#  {
#    n = length(z)
#    out = numeric(R)
#    for (r in 1: R)
#    {
#        p = sample(1:n ,n ,replace = FALSE)
#        out[r] = count5test_permutation(z[p])
#    }
#    sum(out)/R
#  }
#  
#  
#  n1 = 20
#  n2 = 50
#  mu1 = mu2 = 0
#  sigma1 = sigma2 = 1
#  m = 1e3
#  
#  alphahat1 = mean(replicate(m, expr={
#  x = rnorm(n1, mu1, sigma1)
#  y = rnorm(n2, mu2, sigma2)
#  x = x - mean(x) #centered by sample mean
#  y = y - mean(y)
#  count5test(x, y)
#  }))
#  alphahat2 = mean(replicate(m, expr={
#  x = rnorm(n1, mu1, sigma1)
#  y = rnorm(n2, mu2, sigma2)
#  x = x - mean(x) #centered by sample mean
#  y = y - mean(y)
#  z = c(x,y)
#  permutation(z,1000)
#  })<0.05)
#  
#  round(c(count5test=alphahat1,count5test_permutation=alphahat2),4)
#  

## ---- eval=FALSE--------------------------------------------------------------
#  library(RANN)
#  library(boot)
#  library(energy)
#  library(Ball)
#  m <- 1e3
#  k<-3
#  p<-2
#  mu <- 0.5
#  set.seed(12345)
#  fangcha1 <- 1
#  fangcha2 <- 2
#  n1 <- n2 <- 50; R<-999; n <- n1+n2; N = c(n1,n2)
#  Tn <- function(z, ix, sizes,k)
#  {
#    n1 <- sizes[1]; n2 <- sizes[2]; n <- n1 + n2
#    if(is.vector(z)) z <- data.frame(z,0);
#    z <- z[ix, ];
#    NN <- nn2(data=z, k=k+1)
#    block1 <- NN$nn.idx[1:n1,-1]
#    block2 <- NN$nn.idx[(n1+1):n,-1]
#    i1 <- sum(block1 < n1 + .5); i2 <- sum(block2 > n1+.5)
#    (i1 + i2) / (k * n)
#  }
#  eqdist.nn <- function(z,sizes,k)
#  {
#    boot.obj <- boot(data=z,statistic= Tn,R=R,
#                     sim = "permutation", sizes = sizes,k=k)
#    ts <- c(boot.obj$t0,boot.obj$t)
#    p.value <- mean(ts>=ts[1])
#    list(statistic=ts[1],p.value=p.value)
#  }
#  p.values <- matrix(NA,m,3)
#  for(i in 1:m){
#    x <- matrix(rnorm(n1*p,0,fangcha1),ncol=p);
#    y <- cbind(rnorm(n2,0,fangcha2),rnorm(n2,mean=mu,fangcha2));
#    z <- rbind(x,y)
#    p.values[i,1] <- eqdist.nn(z,N,k)$p.value
#    p.values[i,2] <- eqdist.etest(z,sizes=N,R=R)$p.value
#    p.values[i,3] <- bd.test(x=x,y=y,R=999,seed=i*12345)$p.value
#  }
#  alpha <- 0.1;
#  pow <- colMeans(p.values<alpha)
#  print(pow)

## ---- eval=FALSE--------------------------------------------------------------
#  library(RANN)
#  library(boot)
#  library(energy)
#  library(Ball)
#  m <- 1e3
#  k<-3
#  p<-2
#  mu <- 0.5
#  set.seed(12345)
#  n1 <- n2 <- 50; R<-999; n <- n1+n2; N = c(n1,n2)
#  t <-1
#  Tn <- function(z, ix, sizes,k)
#  {
#    n1 <- sizes[1]; n2 <- sizes[2]; n <- n1 + n2
#    if(is.vector(z)) z <- data.frame(z,0);
#    z <- z[ix, ];
#    NN <- nn2(data=z, k=k+1)
#    block1 <- NN$nn.idx[1:n1,-1]
#    block2 <- NN$nn.idx[(n1+1):n,-1]
#    i1 <- sum(block1 < n1 + .5); i2 <- sum(block2 > n1+.5)
#    (i1 + i2) / (k * n)
#  }
#  eqdist.nn <- function(z,sizes,k)
#  {
#    boot.obj <- boot(data=z,statistic= Tn,R=R,
#                     sim = "permutation", sizes = sizes,k=k)
#    ts <- c(boot.obj$t0,boot.obj$t)
#    p.value <- mean(ts>=ts[1])
#    list(statistic=ts[1],p.value=p.value)
#  }
#  p.values <- matrix(NA,m,3)
#  for(i in 1:m){
#    x <- matrix(rt(n1*p,t),ncol=p);
#    y <- cbind(rt(n2,t),rt(n2,t));
#    z <- rbind(x,y)
#    p.values[i,1] <- eqdist.nn(z,N,k)$p.value
#    p.values[i,2] <- eqdist.etest(z,sizes=N,R=R)$p.value
#    p.values[i,3] <- bd.test(x=x,y=y,R=999,seed=i*12345)$p.value
#  }
#  alpha <- 0.1;
#  pow <- colMeans(p.values<alpha)
#  print(pow)

## ---- eval=FALSE--------------------------------------------------------------
#  library(RANN)
#  library(boot)
#  library(energy)
#  library(Ball)
#  m <- 1e3
#  k<-3
#  p<-2
#  mu <- 0.5
#  mu1 <- 0
#  mu2 <- 1
#  D1 <- 1
#  D2 <- 1
#  epsilon <- 0.1
#  fangcha <- (epsilon^2)*D1+((1-epsilon)^2)*D2
#  set.seed(12345)
#  epsilon <- 0.5
#  n1 <- n2 <- 50; R<-999; n <- n1+n2; N = c(n1,n2)
#  Tn <- function(z, ix, sizes,k)
#  {
#    n1 <- sizes[1]; n2 <- sizes[2]; n <- n1 + n2
#    if(is.vector(z)) z <- data.frame(z,0);
#    z <- z[ix, ];
#    NN <- nn2(data=z, k=k+1)
#    block1 <- NN$nn.idx[1:n1,-1]
#    block2 <- NN$nn.idx[(n1+1):n,-1]
#    i1 <- sum(block1 < n1 + .5); i2 <- sum(block2 > n1+.5)
#    (i1 + i2) / (k * n)
#  }
#  eqdist.nn <- function(z,sizes,k)
#  {
#    boot.obj <- boot(data=z,statistic= Tn,R=R,
#                     sim = "permutation", sizes = sizes,k=k)
#    ts <- c(boot.obj$t0,boot.obj$t)
#    p.value <- mean(ts>=ts[1])
#    list(statistic=ts[1],p.value=p.value)
#  }
#  p.values <- matrix(NA,m,3)
#  for(i in 1:m){
#    x <- matrix(rnorm(n1*p,epsilon*mu1+(1-epsilon)*mu2,fangcha),ncol=p);
#    y <- cbind(rnorm(n2,mu2,D2),rnorm(n2,mean=mu2,D2));
#    z <- rbind(x,y)
#    p.values[i,1] <- eqdist.nn(z,N,k)$p.value
#    p.values[i,2] <- eqdist.etest(z,sizes=N,R=R)$p.value
#    p.values[i,3] <- bd.test(x=x,y=y,R=999,seed=i*12345)$p.value
#  }
#  alpha <- 0.1;
#  pow <- colMeans(p.values<alpha)
#  print(pow)

## -----------------------------------------------------------------------------
rm(list=ls())
lamda <- 1/2 # the parameter of standard laplace distribution
N <- 2000
sigma <- c(.05, .5, 2, 16)
x0 <- 2
rw.Metropolis <- function(lamda,sigma,x0,N) 
{
  x <- numeric(N)
  x[1] <- x0
  u <- runif(N)
  k <- 0
  for (i in 2:N) 
    {
    y <- rnorm(1, x[i-1], sigma)
    if (u[i] <= (exp((abs(x[i])-abs(y)))))#the mu is equal to zero so ignore it when computing
      x[i] <- y else {
        x[i] <- x[i-1]
        k <- k + 1
      }
}
  return(list(x=x, k=k))
}
rw1 <- rw.Metropolis(lamda, sigma[1], x0, N)
rw2 <- rw.Metropolis(lamda, sigma[2], x0, N)
rw3 <- rw.Metropolis(lamda, sigma[3], x0, N)
rw4 <- rw.Metropolis(lamda, sigma[4], x0, N)
#number of candidate points rejected
print(c(rw1$k, rw2$k, rw3$k, rw4$k))
#number of acceptance rate
print(c(1-rw1$k/N, 1-rw2$k/N, 1-rw3$k/N, 1-rw4$k/N))

## ---- eval=FALSE--------------------------------------------------------------
#  
#      Gelman.Rubin <- function(psi) {
#          # psi[i,j] is the statistic psi(X[i,1:j])
#          # for chain in i-th row of X
#          psi <- as.matrix(psi)
#          n <- ncol(psi)
#          k <- nrow(psi)
#  
#          psi.means <- rowMeans(psi)     #row means
#          B <- n * var(psi.means)        #between variance est.
#          psi.w <- apply(psi, 1, "var")  #within variances
#          W <- mean(psi.w)               #within est.
#          v.hat <- W*(n-1)/n + (B/n)     #upper variance est.
#          r.hat <- v.hat / W             #G-R statistic
#          return(r.hat)
#          }
#  
#      laplace.chain <- function(sigma, N, X1) {
#          #generates a Metropolis chain for Normal(0,1)
#          #with Normal(X[t], sigma) proposal distribution
#          #and starting value X1
#          x <- rep(0, N)
#          x[1] <- X1
#          u <- runif(N)
#  
#          for (i in 2:N) {
#              xt <- x[i-1]
#              y <- rnorm(1, xt, sigma)     #candidate point
#              r1 <- exp(-abs(y)) * dnorm(xt, y, sigma)
#              r2 <- exp(-abs(xt)) * dnorm(y, xt, sigma)
#              r <- r1 / r2
#              if (u[i] <= r) x[i] <- y else
#                   x[i] <- xt
#              }
#          return(x)
#          }
#  
#      sigma <- 1     #parameter of proposal distribution
#      k <- 4          #number of chains to generate
#      n <- 15000      #length of chains
#      b <- 1000       #burn-in length
#  
#      #choose overdispersed initial values
#      x0 <- c(-10, -5, 5, 10)
#  
#      #generate the chains
#      set.seed(12345)
#      X <- matrix(0, nrow=k, ncol=n)
#      for (i in 1:k)
#          X[i, ] <- laplace.chain(sigma, n, x0[i])
#  
#      #compute diagnostic statistics
#      psi <- t(apply(X, 1, cumsum))
#      for (i in 1:nrow(psi))
#          psi[i,] <- psi[i,] / (1:ncol(psi))
#  
#      #plot psi for the four chains
#  #    par(mfrow=c(2,2))
#      for (i in 1:k)
#        if(i==1){
#          plot((b+1):n,psi[i, (b+1):n],ylim=c(-0.2,0.2), type="l",
#              xlab='Index', ylab=bquote(phi))
#        }else{
#          lines(psi[i, (b+1):n], col=i)
#      }
#      par(mfrow=c(1,1)) #restore default
#  
#      #plot the sequence of R-hat statistics
#      rhat <- rep(0, n)
#      for (j in (b+1):n)
#          rhat[j] <- Gelman.Rubin(psi[,1:j])
#      plot(rhat[(b+1):n], type="l", xlab="", ylab="R")
#      abline(h=1.2, lty=2)

## -----------------------------------------------------------------------------
f = function(a,k){
 
 ck = sqrt(a^2*k/(k+1-a^2))
 l1 = integrate(function(u){(1+u^2/k)^(-(k+1)/2)},0,ck)$value
 l2 = 2/sqrt(pi*k)*exp(lgamma((k+1)/2)-lgamma(k/2))
 l1*l2
}

solve1 = function(k){
  
  output = uniroot(function(a){f(a,k)-f(a,k-1)},lower=1,upper=2)
  output$root
}

k = c(4,25,100,500,1000)

S = function(a,k){

 ck = sqrt(a^2*k/(k+1-a^2))
 pt(ck,df=k,lower.tail=FALSE)
}

solve2 = function(k){
  output = uniroot(function(a){S(a,k)-S(a,k-1)},lower=1,upper=2)
  output$root
}

root2 = matrix(0,2,length(k))

for (i in 1:length(k)){
  root2[2,i]=round(solve2(k[i]),4)
}

root2[1,] = k
rownames(root2) = c('k','A(k)')
root2

## -----------------------------------------------------------------------------

library(nloptr)
# Mle 
eval_f0 = function(x,x1,n.A=444,n.B=132,nOO=361,nAB=63) {
  
  r1 = 1-sum(x1)
  nAA = n.A*x1[1]^2/(x1[1]^2+2*x1[1]*r1)
  nBB = n.B*x1[2]^2/(x1[2]^2+2*x1[2]*r1)
  r = 1-sum(x)
  return(-2*nAA*log(x[1])-2*nBB*log(x[2])-2*nOO*log(r)-
           (n.A-nAA)*log(2*x[1]*r)-(n.B-nBB)*log(2*x[2]*r)-nAB*log(2*x[1]*x[2]))
}


# constraint
eval_g0 = function(x,x1,n.A=444,n.B=132,nOO=361,nAB=63) {
  return(sum(x)-0.999999)
}

opts = list("algorithm"="NLOPT_LN_COBYLA",
             "xtol_rel"=1.0e-8)
mle = NULL
r = matrix(0,1,2)
r = rbind(r,c(0.2,0.35))# the beginning value of p0 and q0
j = 2
while (sum(abs(r[j,]-r[j-1,]))>1e-8) {
res = nloptr( x0=c(0.3,0.25),
               eval_f=eval_f0,
               lb = c(0,0), ub = c(1,1), 
               eval_g_ineq = eval_g0, 
               opts = opts, x1=r[j,],n.A=444,n.B=132,nOO=361,nAB=63)
j = j+1
r = rbind(r,res$solution)
mle = c(mle,eval_f0(x=r[j,],x1=r[j-1,]))
}
#the result of EM algorithm
r 
#the max likelihood values
plot(mle,type = 'l')


## -----------------------------------------------------------------------------

attach(mtcars)

formulas = list(
  mpg ~ disp,
  mpg ~ I(1 / disp),
  mpg ~ disp + wt,
  mpg ~ I(1 / disp) + wt
)
# using loops
result1 = vector("list", length(formulas))
for (i in seq_along(formulas)){
  result1[[i]] = lm(formulas[[i]], data = mtcars)
}
print(result1)
# using lapply
result2 = lapply(formulas, function(x) lm(formula = x, data = mtcars))
print(result2)


## -----------------------------------------------------------------------------
results <- numeric(100)
for(i in 1:100)
{  
x <- rpois(10, 10)
y <- rpois(7, 10)
t <- t.test(x,y)
results[i] <- t$p.value
}
print(results)


## -----------------------------------------------------------------------------

trials = replicate(100,
t.test(rpois(10, 10), rpois(7, 10)),
simplify = FALSE
)
sapply(trials, function(x) x[["p.value"]])

## ----eval=FALSE---------------------------------------------------------------
#      library(Rcpp)
#      library(microbenchmark)
#      # R
#      lap_f = function(x) exp(-abs(x))
#  
#      rw.Metropolis = function(sigma, x0, N){
#      x = numeric(N)
#      x[1] = x0
#      u = runif(N)
#      k = 0
#      for (i in 2:N) {
#      y = rnorm(1, x[i-1], sigma)
#      if (u[i] <= (lap_f(y) / lap_f(x[i-1]))) x[i] = y
#      else {
#      x[i] = x[i-1]
#      k = k+1
#       }
#      }
#       return(list(x = x, k = k))
#      }
#  
#      x0 = 25
#      N = 2000
#      sigma = 2
#      (time = microbenchmark(rwR=rw.Metropolis(sigma,x0,N),rwC=rwMetropolis(sigma,x0,N)))

## ----eval=FALSE---------------------------------------------------------------
#  
#  set.seed(12345)
#  rwR = rw.Metropolis(sigma,x0,N)$x[-(1:500)]
#  rwC = rwMetropolis(sigma,x0,N)[-(1:500)]
#  qqplot(rwR,rwC)
#  abline(a=0,b=1,col='black')

